# -*- coding: utf-8 -*-
"""dl lab3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DqP_Khngi53sY2jV5ZMtdhIWPGJj2dI9
"""

import torch
import torchvision
from torchvision.transforms import functional as f
import cv2
import numpy as np
from google.colab.patches import cv2_imshow
model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)
model.eval()
COCO_LABELS = [
     'background','person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',
    'boat', 'traffic light', 'fire hydrant','none','stop sign', 'parking meter', 'bench', 'bird',
    'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'none', 'backpack',
    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',
    'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'none',
    'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl','none', 'banana', 'apple', 'sandwich', 'orange',
    'broccoli', 'carrot','none', 'hot dog', 'pizza', 'donut', 'cake','none', 'chair', 'couch', 'potted plant', 'bed',
    'dining table', 'toilet', 'none', 'tv', 'laptop', 'mouse', 'remote', 'keyboard','none', 'cell phone','none', 'microwave',
    'oven', 'toaster', 'sink', 'refrigerator','none', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
    'toothbrush'
]


def detect_objects(image_path,confidence_threshold=0.5):
  image=cv2.imread(image_path)
  if image is None:
    print(f"Error loading image from {image_path}")
    return None
  original_image=image.copy()
  image_tensor=f.to_tensor(image)
  with torch.no_grad():
    predictions=model([image_tensor])
  boxes=predictions[0]['boxes'].cpu().numpy()
  labels=predictions[0]['labels'].cpu().numpy()
  scores=predictions[0]['scores'].cpu().numpy()
  for i,box in enumerate(boxes):
    if scores[i]>=confidence_threshold:
      label=coco_instance_category_names[labels[i]]
      score=scores[i]
      start_point=(int(box[0]),int(box[1]))
      end_point=(int(box[2]),int(box[3]))
      cv2.rectangle(original_image,start_point,end_point,(0,255,0),2)
      cv2.putText(original_image,f"{label}:{score:.2f}",start_point,cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,0,0),2)
  return original_image

if __name__=="__main__":
  image_path="/content/dlex14.jpg"
  detected_image=detect_objects(image_path)
  cv2_imshow(detected_image)
  cv2.waitKey(0)
  cv2.destroyAllWindows()

import tensorflow as tf
from tensorflow import keras
model=keras.applications.resnet50.ResNet50(weights='imagenet')
images_resized=tf.image.resize(images,[224,224])
inputs=keras.applications.resnet50.preprocess_input(images_resized*255)
y_proba=model.predict(inputs)\
top_k=keras.application.resnet50.decode_predictions(y_proba,top=3)
for image_index in range(len(images)):
  print("image #{}".format(image_index))
  for class_id,name,y_proba in top_k[image_index]:
    print(" {} - {:12s} {:.2f}%".format(class_id,name,y_proba*100))
  print()

from torchvision import transforms

# Define augmentations
augmentation = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((300, 300)),  # Resize to 300x300
    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip
    transforms.RandomRotation(180),  # Rotate by Â±15 degrees
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random brightness and color adjustments
    transforms.ToTensor()
])

def detect_objects_with_augmentation(image_path, confidence_threshold=0.5):
    # Load image
    image = cv2.imread(image_path)
    if image is None:
        print(f"Error loading image from {image_path}")
        return None

    # Apply augmentation
    augmented_image = augmentation(image)

    # Perform detection
    with torch.no_grad():
        predictions = model([augmented_image])

    # Extract predictions
    boxes = predictions[0]['boxes'].cpu().numpy()
    labels = predictions[0]['labels'].cpu().numpy()
    scores = predictions[0]['scores'].cpu().numpy()

    # Draw boxes on the original image
    original_image = image.copy()
    for i, box in enumerate(boxes):
        if scores[i] >= confidence_threshold:
            label = coco_instance_category_names[labels[i]]
            score = scores[i]
            start_point = (int(box[0]), int(box[1]))
            end_point = (int(box[2]), int(box[3]))
            cv2.rectangle(original_image, start_point, end_point, (0, 255, 0), 2)
            cv2.putText(original_image, f"{label}: {score:.2f}", start_point, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

    return original_image



if __name__=="__main__":
  image_path="/content/dlex.png"
  detected_image=detect_objects(image_path)
  cv2_imshow(detected_image)
  cv2.waitKey(0)
  cv2.destroyAllWindows()

import os
import torch
import torchvision
from torchvision.transforms import functional as f
import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Load Pretrained Model
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()

# COCO Class Names
coco_instance_category_names = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',
    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',
    'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',
    'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',
    'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',
    'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',
    'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

# Function to detect objects in a single image
def detect_objects(image_path, confidence_threshold=0.5):
    image = cv2.imread(image_path)
    if image is None:
        print(f"Error loading image from {image_path}")
        return None
    original_image = image.copy()
    image_tensor = f.to_tensor(image)
    with torch.no_grad():
        predictions = model([image_tensor])
    boxes = predictions[0]['boxes'].cpu().numpy()
    labels = predictions[0]['labels'].cpu().numpy()
    scores = predictions[0]['scores'].cpu().numpy()

    # Annotate detected objects
    for i, box in enumerate(boxes):
        if scores[i] >= confidence_threshold:
            label_index = labels[i]-1
            if label_index < len(coco_instance_category_names):  # Ensure index is valid
                label = coco_instance_category_names[label_index]
                score = scores[i]
                start_point = (int(box[0]), int(box[1]))
                end_point = (int(box[2]), int(box[3]))
                cv2.rectangle(original_image, start_point, end_point, (0, 255, 0), 2)
                cv2.putText(original_image, f"{label}: {score:.2f}", start_point,
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
            else:
                print(f"Warning: Label index {label_index} is out of bounds.")
    return original_image


# Batch processing function
def process_batch(input_folder, output_folder, confidence_threshold=0.5):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    image_files = [f for f in os.listdir(input_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]

    for i, image_file in enumerate(image_files):
        image_path = os.path.join(input_folder, image_file)
        print(f"Processing {i + 1}/{len(image_files)}: {image_path}")

        # Detect objects
        detected_image = detect_objects(image_path, confidence_threshold)

        if detected_image is not None:
            # Save annotated image
            output_path = os.path.join(output_folder, image_file)
            cv2.imwrite(output_path, detected_image)

# Main function
if __name__ == "__main__":
    input_folder ="/content/dla" # Replace with your input folder
    output_folder ="/content/dlao2"  # Replace with your output folder
    confidence_threshold = 0.7

    process_batch(input_folder, output_folder, confidence_threshold)

    print(f"Processing complete. Annotated images saved to {output_folder}")

import os
import torch
import torchvision
from torchvision.transforms import functional as f
import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Load the pre-trained Faster R-CNN model
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()

# COCO dataset class labels

coco_instance_category_names = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',
    'parking meter', 'bench', 'bird', 'none','cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',
    'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',
    'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',
    'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',
    'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',
    'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]
# Resize function
def resize_image(image, target_size=(800, 800)):
    return cv2.resize(image, target_size)

def detect_objects_batch(image_folder, confidence_threshold=0.5):
    # List all image files in the folder
    image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]

    if len(image_paths) == 0:
        print(f"No images found in folder: {image_folder}")
        return []

    images = []
    for image_path in image_paths:
        image = cv2.imread(image_path)
        if image is None:
            print(f"Error loading image from {image_path}")
            continue
        images.append(image)

    if len(images) == 0:
        return []

    # Resize images to the same size
    target_size = (800, 800)  # Define your desired size
    resized_images = [resize_image(image, target_size) for image in images]

    # Convert list of images to list of tensors
    image_tensors = [f.to_tensor(image) for image in resized_images]

    # Stack the tensors to create a batch
    batch = torch.stack(image_tensors)

    with torch.no_grad():
        predictions = model(batch)

    # Process predictions for each image in the batch
    output_images = []
    for i, original_image in enumerate(resized_images):
        prediction = predictions[i]
        boxes = prediction['boxes'].cpu().numpy()
        labels = prediction['labels'].cpu().numpy()
        scores = prediction['scores'].cpu().numpy()

        for j, box in enumerate(boxes):
            if scores[j] >= confidence_threshold:
                label = coco_instance_category_names[labels[j]]
                score = scores[j]
                start_point = (int(box[0]), int(box[1]))
                end_point = (int(box[2]), int(box[3]))
                cv2.rectangle(original_image, start_point, end_point, (0, 255, 0), 2)
                cv2.putText(original_image, f"{label}:{score:.2f}", start_point, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        output_images.append(original_image)

    return output_images

if __name__ == "__main__":
    # Example: Batch processing of images from a folder
    image_folder = "/content/dla"  # Replace with your folder path
    detected_images = detect_objects_batch(image_folder)

    # Display each processed image
    for detected_image in detected_images:
        cv2_imshow(detected_image)  # This works in Google Colab to display images

import os
import torch
import torchvision
from torchvision.transforms import functional as f
import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Load Pretrained Model
model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)

model.eval()

# COCO Class Names
coco_instance_category_names = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',
    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',
    'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',
    'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',
    'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',
    'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',
    'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

# Function to detect objects in a single image
from google.colab.patches import cv2_imshow

def detect_objects(image_path, confidence_threshold=0.5):
    image = cv2.imread(image_path)
    if image is None:
        print(f"Error loading image from {image_path}")
        return None
    original_image = image.copy()
    image_tensor = f.to_tensor(image)
    with torch.no_grad():
        predictions = model([image_tensor])

    boxes = predictions[0]['boxes'].cpu().numpy()
    labels = predictions[0]['labels'].cpu().numpy()
    scores = predictions[0]['scores'].cpu().numpy()

    # Print raw predictions to debug
    print(f"Raw Predictions: {len(boxes)} boxes detected")
    print(f"Boxes: {boxes}")
    print(f"Labels: {labels}")
    print(f"Scores: {scores}")

    # Annotate detected objects
    for i, box in enumerate(boxes):
        if scores[i] >= confidence_threshold:
            label_index = labels[i] - 1
            if label_index < len(coco_instance_category_names):  # Ensure index is valid
                label = coco_instance_category_names[label_index]
                score = scores[i]
                start_point = (int(box[0]), int(box[1]))
                end_point = (int(box[2]), int(box[3]))
                cv2.rectangle(original_image, start_point, end_point, (0, 255, 0), 2)
                cv2.putText(original_image, f"{label}: {score:.2f}", start_point,
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
            else:
                print(f"Warning: Label index {label_index} is out of bounds.")
    return original_image




# Batch processing function
def process_batch(input_folder, output_folder, confidence_threshold=0.5):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    image_files = [f for f in os.listdir(input_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]

    for i, image_file in enumerate(image_files):
        image_path = os.path.join(input_folder, image_file)
        print(f"Processing {i + 1}/{len(image_files)}: {image_path}")

        # Detect objects
        detected_image = detect_objects(image_path, confidence_threshold)

        if detected_image is not None:
            # Save annotated image
            output_path = os.path.join(output_folder, image_file)
            cv2.imwrite(output_path, detected_image)

# Main function
if __name__ == "__main__":
    input_folder ="/content/dla" # Replace with your input folder
    output_folder ="/content/lao3"  # Replace with your output folder
    confidence_threshold = 0.5

    process_batch(input_folder, output_folder, confidence_threshold)

    print(f"Processing complete. Annotated images saved to {output_folder}")

# Set the confidence threshold lower (e.g., 0.3) to keep more predictions
confidence_threshold = 0.3

# Updated detection function with lower threshold
def detect_objects_batch(image_folder, ground_truths, confidence_threshold=0.3, iou_threshold=0.3):
    image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

    if not image_paths:
        print(f"No images found in folder: {image_folder}")
        return [], []

    images = [cv2.imread(image_path) for image_path in image_paths if cv2.imread(image_path) is not None]
    if not images:
        print("No valid images to process.")
        return [], []

    resized_images = [resize_image(image) for image in images]
    image_tensors = [F.to_tensor(image) for image in resized_images]
    batch = torch.stack(image_tensors)

    with torch.no_grad():
        predictions = model(batch)

    true_positive = []
    pred_scores = []
    pred_labels = []
    output_images = []

    for i, original_image in enumerate(resized_images):
        prediction = predictions[i]
        boxes = prediction['boxes'].cpu().numpy()
        labels = prediction['labels'].cpu().numpy()
        scores = prediction['scores'].cpu().numpy()

        if i not in ground_truths:
            print(f"No ground truth available for image {i}, skipping...")
            continue

        gt_boxes = ground_truths[i]['boxes']
        gt_labels = ground_truths[i]['labels']

        for j, box in enumerate(boxes):
            if scores[j] >= confidence_threshold:
                pred_scores.append(scores[j])
                pred_labels.append(labels[j])
                iou_values = [calculate_iou(box, gt_box) for gt_box in gt_boxes]

                # Print IoU values for debugging
                print(f"Image {i}, Prediction {j}, IoUs: {iou_values}")

                max_iou = max(iou_values) if iou_values else 0
                if max_iou >= iou_threshold:
                    true_positive.append(1)
                else:
                    true_positive.append(0)

                label = COCO_LABELS[labels[j]]
                start_point = (int(box[0]), int(box[1]))
                end_point = (int(box[2]), int(box[3]))
                cv2.rectangle(original_image, start_point, end_point, (0, 255, 0), 2)  # Predicted boxes in green
                cv2.putText(original_image, f"{label}:{scores[j]:.2f}", start_point, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        # Draw ground truth boxes in red
        for gt_box in gt_boxes:
            start_point = (int(gt_box[0]), int(gt_box[1]))
            end_point = (int(gt_box[2]), int(gt_box[3]))
            cv2.rectangle(original_image, start_point, end_point, (0, 0, 255), 2)  # Ground truth boxes in red

        output_images.append(original_image)

    if pred_scores and true_positive:
        mAP = average_precision_score(true_positive, pred_scores)
    else:
        print("No valid predictions for mAP calculation.")
        mAP = 0

    print(f"mAP: {mAP:.4f}")
    return output_images, mAP

# Run the detection again with the lower confidence threshold
detected_images, mAP = detect_objects_batch(image_folder, ground_truths)
for detected_image in detected_images:
    cv2_imshow(detected_image)  # Visualize the detected images with bounding boxes

